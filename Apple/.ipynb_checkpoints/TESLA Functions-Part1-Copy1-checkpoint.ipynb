{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['\\xa0', 'March\\xa031, 2020', '\\xa0', 'December\\xa031, 2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date( column ):\n",
    "    \n",
    "    date = []\n",
    "    \n",
    "    for ele_num, ele in enumerate(column):\n",
    "        \n",
    "        if ele != '\\xa0':\n",
    "            \n",
    "            date_data = re.sub('({})'.format('|'.join(map(re.escape, month_dict.keys()))), lambda m: month_dict[m.group()], ele)\n",
    "\n",
    "            pattern = re.compile(r'\\w+')\n",
    "\n",
    "            res = pattern.findall(date_data)\n",
    "            #print('res', '-' , res)\n",
    "\n",
    "            date.append(res[0])\n",
    "\n",
    "    date.insert(0, 'Quarter')\n",
    "    \n",
    "    print('\\n', date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res - ['Q1', '31', '2020']\n",
      "res - ['Q4', '31', '2019']\n",
      "\n",
      " ['Quarter', 'Q1', 'Q4']\n"
     ]
    }
   ],
   "source": [
    "date(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year( column ):\n",
    "    \n",
    "    years = []\n",
    "    \n",
    "    for ele_num, ele in enumerate(column):\n",
    "        \n",
    "        if ele != '\\xa0':\n",
    "            \n",
    "            clean_year = re.sub('({})'.format('|'.join(map(re.escape, month_dict.keys()))), lambda m: month_dict[m.group()], ele)\n",
    "\n",
    "            pattern = re.compile(r'\\w+')\n",
    "\n",
    "            res = pattern.findall(clean_year)\n",
    "\n",
    "            years.append(res[2])\n",
    "\n",
    "    years.insert(0, 'Year')\n",
    "    \n",
    "    print('\\n', years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['Year', '2020', '2019']\n"
     ]
    }
   ],
   "source": [
    "year(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my capstone project I want to try to use machine learning to aggregate information from different financial analytical approaches \n",
    "\n",
    "Tesla's disruption to the automotive industry began back in 2008 with its exclusive highend Roadster () up until the recent reveal of the mainstream Model Y ().\n",
    "\n",
    "As the pioneer and leader in EV sales worldwide (), Tesla perhaps has gained more attention in its financial sustainability (); as of this writing, Tesla has yet produced a profittable quarter(). This ***uncertainity of unprofitablity*** has resonated throughout the EV market and automotive market in large, which in turn made Tesla to become one of the most important benchmark indicator of the EV story (). \n",
    "\n",
    "~~~~~\n",
    "\n",
    "This study aims to explore whether Tesla's business model is more related to traditional automakers or more closely related to tech companies. We will use a multi-class classification \n",
    "\n",
    "In this report I will attempt to predict the Tesla's stock price using different financial and statistical models. In the end I will discuss the performance of each model and some future recommendations. \n",
    "\n",
    "~~~~~\n",
    "\n",
    "### **Investment Horizon of Models** \n",
    "\n",
    "1) **Intrinsic Value** Long Term (Quarters/Years)\n",
    "\n",
    "2) **Discounted Cash Flow** Mid Term (Quarters)\n",
    "\n",
    "3) **Comparaable Analysis** Mid Term (Days/Months)\n",
    "\n",
    "4) **Time Series** Short (Days)\n",
    "\n",
    "    - Time Autoregressive Integrated Moving Average (ARIMA)\n",
    "     https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3168423\n",
    "\n",
    "Model Evalution\n",
    "AUROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library \n",
    "\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd \n",
    "import csv\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Text Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "    Replace C1 control characters in the Unicode string s by the\n",
    "    characters at the corresponding code points in Windows-1252,\n",
    "    where possible.\n",
    "    \"\"\"\n",
    "\n",
    "def to_windows_1252(match):\n",
    "    try:\n",
    "        return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "    except UnicodeDecodeError:\n",
    "        # No character at the corresponding code point: remove it.\n",
    "        return ''\n",
    "\n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the Document Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_doc_content( brand, CIK ):\n",
    "    \n",
    "    company = {}\n",
    "    \n",
    "    company['auto'] = {}\n",
    "    auto = {brand : CIK\n",
    "           }\n",
    "    company['auto'] = auto \n",
    "\n",
    "    key = list(auto.copy().values())\n",
    "    \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0001318605']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grab_doc_content('Tesla', '0001318605')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_filings_dict = {}\n",
    "    \n",
    "file_code = {}\n",
    "    \n",
    "file_text = {}\n",
    "\n",
    "Accession_Number_URL = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Directory For CIK    \n",
    "dir_url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-&dateb=&owner=include&count=100'\n",
    "dir_url_list = [dir_url.format(x) for x in grab_doc_content('Tesla', '0001318605')]\n",
    "\n",
    "#print('Directory URL: {}'.format(dir_url_list))\n",
    "doc_url_list = [] \n",
    "\n",
    "# FOR-loop yielding Accession Numbers from CIK/URL Directory.\n",
    "for CIK_num in grab_doc_content('Tesla', '0001318605'):\n",
    "\n",
    "    doc_url = r'https://www.sec.gov/Archives/edgar/data/{CIKx}/{xx}/{yy}.txt'\n",
    "    doc_url_new = doc_url.format(CIKx = CIK_num, xx='{xx}', yy ='{yy}')\n",
    "    doc_url_list.append(doc_url_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_maker(dir_url):\n",
    "    dir_url_list = [dir_url.format(x) for x in grab_doc_content('Tesla', '0001318605')]\n",
    "\n",
    "    #print('Directory URL: {}'.format(dir_url_list))\n",
    "    doc_url_list = [] \n",
    "\n",
    "    # FOR-loop yielding Accession Numbers from CIK/URL Directory.\n",
    "    for CIK_num in grab_doc_content('Tesla', '0001318605'):\n",
    "\n",
    "        doc_url = r'https://www.sec.gov/Archives/edgar/data/{CIKx}/{xx}/{yy}.txt'\n",
    "        doc_url_new = doc_url.format(CIKx = CIK_num, xx='{xx}', yy ='{yy}')\n",
    "        \n",
    "        doc_url_list.append(doc_url_new)\n",
    "        \n",
    "        url_lists = zip( dir_url_list , doc_url_list )\n",
    "        \n",
    "    return url_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001318605&type=10-&dateb=&owner=include&count=100\n",
      "2 https://www.sec.gov/Archives/edgar/data/0001318605/{xx}/{yy}.txt\n"
     ]
    }
   ],
   "source": [
    "url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-&dateb=&owner=include&count=100'\n",
    "for dir_url, doc_url in url_maker(url): \n",
    "    print(1, dir_url)\n",
    "    print(2, doc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accession_Number_URL = {}\n",
    "url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-&dateb=&owner=include&count=100'\n",
    "\n",
    "for dir_url, doc_url in url_maker(url):\n",
    "    \n",
    "    response = requests.get(dir_url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    text = soup.get_text(strip=True)\n",
    "\n",
    "    cleaned_text = re.findall('Acc-no: \\d+-\\d+-\\d+' , text)\n",
    "\n",
    "    accession_number = [n.replace('Acc-no: ', '') for n in cleaned_text]\n",
    "    accessionnumber = [num.replace('-', '') for num in accession_number]\n",
    "\n",
    "    accession_numbers = zip(accessionnumber, accession_number)\n",
    "    \n",
    "    cikk = [ cikk_.replace('CIK=', '') for cikk_ in re.findall('CIK=\\d+', dir_url) ][0]\n",
    "    CIKK = {cikk : accessionnumber}\n",
    "    \n",
    "    for (a,b) in accession_numbers: \n",
    "        master_filings_dict[b] = {}\n",
    "        master_filings_dict[b]['sec_header_content'] = {}\n",
    "        master_filings_dict[b]['filing_documents'] = None\n",
    "\n",
    "        doc_url_single = doc_url.format(xx = a, yy = b)\n",
    "\n",
    "        file_url_list = []\n",
    "\n",
    "        \n",
    "\n",
    "        file_url_list.append( doc_url_single )\n",
    "        \n",
    "        Accession_Number_URL.update({ b : file_url_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001564590-20-019931', '0001564590-20-018984', '0001564590-20-004475', '0001564590-19-038256', '0001564590-19-026445', '0001564590-19-013462', '0001564590-19-003165', '0001564590-18-026353', '0001564590-18-019254', '0001564590-18-011086', '0001564590-18-002956', '0001564590-17-021343', '0001564590-17-015705', '0001564590-17-009968', '0001564590-17-003118', '0001564590-16-026820', '0001564590-16-023024', '0001564590-16-018886', '0001564590-16-013195', '0001564590-15-009741', '0001564590-15-006666', '0001564590-15-003789', '0001564590-15-001031']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "A_N = []\n",
    "for AN, url in Accession_Number_URL.items():\n",
    "    \n",
    "    if AN == '0001193125-14-403635':\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        A_N.append(AN)\n",
    "print(A_N)\n",
    "print(len(A_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each Filing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 71 page(s) found.\n",
      "There was 71 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-20-019931 were parsed and stored.\n"
     ]
    }
   ],
   "source": [
    "QorK = {'10-Q' : [],\n",
    "        '10-K' : []\n",
    "       }\n",
    "\n",
    "for acc_num, url in Accession_Number_URL.items():\n",
    "    \n",
    "    master_document_dict = {}\n",
    "    \n",
    "    # create a stop point\n",
    "    if acc_num == '0001564590-20-018984':\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # grab the response\n",
    "        response = requests.get(url[0])\n",
    "\n",
    "    # Soupify\n",
    "        # pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        filing_document = soup.find('document')\n",
    "        \n",
    "    # Parsing\n",
    "        # Document type ->> document_id\n",
    "        document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "        # Document sequence ->> document_sequence\n",
    "        document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "        # Document filename ->> document_filename\n",
    "        document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "        # Document description ->> document_description\n",
    "        document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "        \n",
    "    # Storage    \n",
    "        # initalize our document dictionary\n",
    "        master_document_dict[document_id] = {}\n",
    "\n",
    "        # add the different parts, we parsed up above.\n",
    "        master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "        master_document_dict[document_id]['document_filename'] = document_filename\n",
    "        master_document_dict[document_id]['document_description'] = document_description\n",
    "        \n",
    "# Scraping\n",
    "        # grab the text portion of the document, this will be used to split the document into pages.\n",
    "        filing_doc_text = filing_document.find('text').extract()\n",
    "\n",
    "        # find all the thematic breaks, these help define page numbers and page breaks.\n",
    "        all_thematic_breaks = filing_doc_text.find_all('hr')\n",
    "\n",
    "        # Locate and store page number via list comprehension.\n",
    "        all_page_numbers = [thematic_break.previous_sibling.previous_sibling.get_text(strip=True) for thematic_break in all_thematic_breaks]\n",
    "\n",
    "        # convert all thematic breaks to a string so it can be used for parsing\n",
    "        all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "\n",
    "        # prep the document text for splitting, this means converting it to a string.\n",
    "        filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "        # handle the case where there are thematic breaks.\n",
    "        if len(all_thematic_breaks) > 0: \n",
    "\n",
    "            # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "            regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "\n",
    "            # split the document along each thematic break.\n",
    "            split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "            # store the document itself\n",
    "            master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "\n",
    "        # handle the case where there are no thematic breaks.\n",
    "        elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "            # handles so it will display correctly.\n",
    "            split_filing_string = all_thematic_breaks\n",
    "\n",
    "            # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "            master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "            \n",
    "        # display some information to the user.\n",
    "        print('-'*80)\n",
    "        print('The document {} was parsed.'.format(document_id))\n",
    "        print('There was {} page(s) found.'.format(len(all_page_numbers)))\n",
    "        print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "\n",
    "        # store the documents in the master_filing_dictionary.\n",
    "        master_filings_dict[acc_num]['filing_documents'] = master_document_dict\n",
    "        \n",
    "        # if document is 10-Q\n",
    "        if document_id == '10-Q':\n",
    "            QorK['10-Q'].append(acc_num) # add acc_num to QorK in 10-Q as key\n",
    "        \n",
    "        # if document is 10-K\n",
    "        if document_id == '10-K':\n",
    "            QorK['10-K'].append(acc_num) # add acc_num to QorK in 10-K as key\n",
    "        \n",
    "        del master_document_dict\n",
    "        \n",
    "        print('-'*80)\n",
    "        print('All the documents for filing {} were parsed and stored.'.format(acc_num))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_test = master_filings_dict['0001564590-20-019931']['filing_documents']['10-Q']['pages_code']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Check Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variations of HTML anchors.\n",
    "CBS_A = '<a name=\"Consolidated_Balance_Sheets'\n",
    "CBS_B = 'id=\"CONSOLIDATED_BALANCE_SHEETS'\n",
    "CBS3 = 'name=\"CONSOLIDATED_BALANCE_SHEETS\"' \n",
    "CBS10Knew = 'id=\"Consolidated_Balance_Sheets\"'\n",
    "\n",
    "SoO_A = '<a name=\"Statements_of_Operations\"' \n",
    "SoO_B = 'id=\"CONSOLIDATED_STATEMENTS_OPERATIONS\"'\n",
    "SoO3 = 'name=\"CONSOLIDATED_STATEMENTS_OPERATIONS\"'\n",
    "CSoO10K = 'name=\"Consolidated_Statements_of_Operations\"'\n",
    "CSoO10Knew = 'id=\"Consolidated_Statements_of_Operations\"'\n",
    "\n",
    "CF_A = '<a name=\"Statements_of_Cash'\n",
    "CF_B = 'id=\"CONSOLIDATED_STATEMENTS_CASH_FLOWS\"'\n",
    "CF3 = 'name=\"CONSOLIDATED_STATEMENTS_CASH_FLOWS\"'\n",
    "CCF10K = 'name=\"Consolidated_Statements_of_Cash_Flows\"'\n",
    "CCF10Knew = 'id=\"Consolidated_Statements_of_Cash_Flows\"'\n",
    "\n",
    "def anchor_check(page_code):\n",
    "    \n",
    "    # if the page has one of these anchors\n",
    "    if (CBS_A in page_code) or (CBS_B in page_code) or (CBS3 in page_code) or (CBS10Knew in page_code):\n",
    "        return True\n",
    "\n",
    "    #elif (SoO_A in page_code) or (SoO_B in page_code) or (SoO3 in page_code) or (CSoO10K in page_code) or (CSoO10Knew in page_code) :\n",
    "        #return True\n",
    "\n",
    "    #elif (CF_A in page_code) or (CF_B in page_code) or (CF3 in page_code) or (CCF10K in page_code) or (CCF10Knew in page_code):\n",
    "        #return True\n",
    "\n",
    "\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving string test to local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're looking at the following file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string_test = master_filings_dict['0001564590-20-019931']['filing_documents']['10-Q']['pages_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stringtest.csv\", \"wb\") as fp:\n",
    "    pickle.dump(string_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading string test from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"stringtest.csv\", \"rb\") as fp:\n",
    "    string_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Columns Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dict = {'December' : 'Q4',\n",
    "              'November' : 'Q4',\n",
    "             'October' : 'Q4',\n",
    "             'September' : 'Q3',\n",
    "             'August': 'Q3',\n",
    "             'July' : 'Q3',\n",
    "             'June' : 'Q2',\n",
    "             'May' : 'Q2',\n",
    "             'April' : 'Q2',\n",
    "             'March' : 'Q1',\n",
    "             'Feburary' : 'Q1',\n",
    "             'January' : 'Q1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortening Column name.\n",
    "def clean_col_name( element ):\n",
    "    \n",
    "    if re.findall('Common stock.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Common Stock') \n",
    "\n",
    "        return element\n",
    "        \n",
    "    elif re.findall('Preferred stock.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Preferred Stock') \n",
    "        \n",
    "        return element\n",
    "    \n",
    "    elif re.findall(\"Total liabilities and stockholders' equity.*\", element):\n",
    "        \n",
    "        element = element.replace(element, 'Total liabilities and equity' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Accounts receivable.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Accounts receivable' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Accrued liabilities.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Accrued Liabilities' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Deferred revenue.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Deferred Revenue' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Redeemable noncontrolling interests in subsidiaries.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Noncontrolling interests in subsidiaries' ) \n",
    "        \n",
    "        return element\n",
    "    \n",
    "    elif re.findall('Convertible senior notes.*', element, flags=re.I):\n",
    "        #print('before: ', element)\n",
    "        element = element.replace(element, 'Convertible Senior Notes' )\n",
    "        #print('after: ', element)\n",
    "        return element\n",
    "    \n",
    "    else:\n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw column list-like object into cleaning function. \n",
    "def cleaning_column(column, m):\n",
    "     \n",
    "    column_list = []\n",
    "\n",
    "    for element_post, element in enumerate(column):\n",
    "        \n",
    "        # Handling the first \"row\" in the table i.e. column name.\n",
    "        if element_post == 0:\n",
    "\n",
    "            element = unicodedata.normalize('NFKD', element)\n",
    "            \n",
    "            element = element.replace( '\\n', '')\n",
    "            \n",
    "            clean_ele = clean_col_name( element )\n",
    "            \n",
    "            column_list.append(clean_ele)\n",
    "            \n",
    "        # Use a dictionary to convert the Months into numbers.\n",
    "        #elif element_post == :\n",
    "            #month = re.sub('({})'.format('|'.join(map(re.escape, month_dict.keys()))), lambda m: month_dict[m.group()], dat)\n",
    "            #column_list.extend(month)\n",
    "        #elif element == '(' or ')': \n",
    "            #pint(element)\n",
    "            #continue\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            element = element.replace( '\\n$' and '\\n\\xa0' and '\\n', '0')\n",
    "            \n",
    "            element = element.replace( ',', '' )\n",
    "\n",
    "            pattern = re.compile(r'[\\d]+,[\\d]+|[\\d]+')\n",
    "\n",
    "            res = pattern.findall(element)\n",
    "\n",
    "            res = [int(ele) * m for ele in res]\n",
    "\n",
    "            column_list.extend(res)\n",
    "\n",
    "    return column_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tables Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date( column ):\n",
    "    \n",
    "    date = []\n",
    "    \n",
    "    for ele in column:\n",
    "        \n",
    "        norm_ele = unicodedata.normalize('NFKD', ele)\n",
    "        \n",
    "        date_data = re.sub('({})'.format('|'.join(map(re.escape, month_dict.keys()))), lambda m: month_dict[m.group()], norm_ele)\n",
    "\n",
    "        element = date_data.replace( '\\n' and ' ' and '\\n ', '0')\n",
    "        \n",
    "        pattern = re.compile(r'\\w+')\n",
    "\n",
    "        res = pattern.findall(element)\n",
    "        \n",
    "        res = str(res[0]) \n",
    "\n",
    "        date.append(res)\n",
    "\n",
    "    date.pop(-1)\n",
    "\n",
    "    date.insert(0, 'Quarter')\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year( column ):\n",
    "    \n",
    "    years = []\n",
    "                \n",
    "    for year in column:\n",
    "\n",
    "        norm_year = unicodedata.normalize('NFKD', year)\n",
    "\n",
    "        clean_year = norm_year.replace( '\\n', '0')\n",
    "\n",
    "        years.append(str(int(clean_year)))\n",
    "        \n",
    "    years.pop(-1)\n",
    "\n",
    "    years.insert(0, 'Year')\n",
    "    \n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_extractor( page, multiplier ):\n",
    "    \n",
    "    # Convert string to BS object\n",
    "    soup = BeautifulSoup(page, 'html5')\n",
    "\n",
    "    # then get all the rows in the table.\n",
    "    table_rows = soup.find_all('tr')\n",
    "    \n",
    "    if len( [tr.text for tr in table_rows[0].find_all('td')] ) < 3:\n",
    "        del table_rows[0]\n",
    "    \n",
    "    single_table = []\n",
    "    \n",
    "    # Rotate through each column, adding in three structures to single_table.\n",
    "    for tr_post, tr in enumerate(table_rows):\n",
    "\n",
    "        td = tr.find_all('td')\n",
    "\n",
    "        column = [tr.text for tr in td]\n",
    "        print( len(column), column)\n",
    "        if tr_post == 0:\n",
    "            \n",
    "            col_date = date( column )\n",
    "\n",
    "            single_table.append( col_date )\n",
    "         \n",
    "        elif tr_post == 1: \n",
    "            \n",
    "            col_year = year( column )\n",
    "            \n",
    "            single_table.append( col_year )\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            cleaned_data = cleaning_column( column , multiplier )\n",
    "            \n",
    "            if len(cleaned_data) > 10:\n",
    "                del cleaned_data[3]\n",
    "            \n",
    "            single_table.append( cleaned_data )\n",
    "            \n",
    "    return single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_clean( single_table ):\n",
    "    #print(single_table)\n",
    "    table_df = pd.DataFrame(single_table)\n",
    "\n",
    "    table_df = table_df.transpose()  \n",
    "\n",
    "    table_df.drop(table_df.index[1:3], 0, inplace=True)\n",
    "\n",
    "    table_df.drop(table_df.index[2:], 0, inplace=True)\n",
    "\n",
    "    table_df.columns = table_df.iloc[0]\n",
    "\n",
    "    table_df = table_df.drop(table_df.index[0])\n",
    "    \n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table(page_code):\n",
    "    \n",
    "    doc_tables=[]\n",
    "\n",
    "    for page_post, page in enumerate(page_code):\n",
    "        \n",
    "        if anchor_check(page) and ('in millions' in page):\n",
    "            \n",
    "            single_table = table_extractor( page, 10**6 )\n",
    "            \n",
    "            table_df = df_clean(single_table)\n",
    "                        \n",
    "            doc_tables.append(table_df)\n",
    "        \n",
    "        elif anchor_check(page) and ('in thousands' in page):\n",
    "            \n",
    "            single_table = table_extractor( page, 10**3 )\n",
    "            \n",
    "            table_df = df_clean(single_table)\n",
    "            \n",
    "            doc_tables.append(table_df)\n",
    "                \n",
    "    return doc_tables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Facebookstring_test = master_filings_dict['0001326801-20-000013']['filing_documents']['10-K']['pages_code']\n",
    "for page in pages:\n",
    "\n",
    "    if  anchor_check(page) and ('In millions' in page):\n",
    "        print('\\n', 'Pass Anchor check')\n",
    "\n",
    "        # Convert string to BS object\n",
    "        soup = BeautifulSoup(page, 'html5')\n",
    "\n",
    "        # then get all the rows in the table.\n",
    "        table_rows = soup.find_all('tr')\n",
    "\n",
    "        if len( [tr.text for tr in table_rows[0].find_all('td')] ) < 3:\n",
    "            del table_rows[0]\n",
    "            print('another table row deleted', '\\n')\n",
    "        single_table = []\n",
    "\n",
    "        # Loop through each column, adding in three structures to single_table.\n",
    "        for tr_post, tr in enumerate(table_rows):\n",
    "\n",
    "            td = tr.find_all('td')\n",
    "\n",
    "            column = [tr.text for tr in td]\n",
    "            #print( len(column), column)\n",
    "            if tr_post == 1:\n",
    "\n",
    "                col_date = Quarter( column )\n",
    "                single_table.append( col_date )\n",
    "            \n",
    "            if tr_post == 2:\n",
    "                col_year = year( column )\n",
    "                single_table.append( col_year )\n",
    "\n",
    "            elif tr_post > 4: \n",
    "\n",
    "                cleaned_data = cleaning_column( column , 10**6 )\n",
    "\n",
    "                if len(cleaned_data) > 10:\n",
    "                    del cleaned_data[3]\n",
    "\n",
    "                single_table.append( cleaned_data )\n",
    "\n",
    "        table_df = df_clean(single_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 ['\\n\\xa0', '\\n\\xa0', '\\nMarch\\xa031,', '\\n\\xa0', '\\n\\xa0', '\\nDecember 31,', '\\n\\xa0']\n",
      "7 ['\\n\\xa0', '\\n\\xa0', '\\n2020', '\\n\\xa0', '\\n\\xa0', '\\n2019', '\\n\\xa0']\n",
      "9 ['\\nAssets', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 ['\\nCurrent assets', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 ['\\nCash and cash equivalents', '\\n\\xa0', '\\n$', '\\n8,080', '\\n\\xa0', '\\n\\xa0', '\\n$', '\\n6,268', '\\n\\xa0']\n",
      "9 ['\\nAccounts receivable, net', '\\n\\xa0', '\\n\\xa0', '\\n1,274', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n1,324', '\\n\\xa0']\n",
      "9 ['\\nInventory', '\\n\\xa0', '\\n\\xa0', '\\n4,494', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n3,552', '\\n\\xa0']\n",
      "9 ['\\nPrepaid expenses and other current assets', '\\n\\xa0', '\\n\\xa0', '\\n1,045', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n959', '\\n\\xa0']\n",
      "9 ['\\nTotal current assets', '\\n\\xa0', '\\n\\xa0', '\\n14,893', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n12,103', '\\n\\xa0']\n",
      "9 ['\\nOperating lease vehicles, net', '\\n\\xa0', '\\n\\xa0', '\\n2,527', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n2,447', '\\n\\xa0']\n",
      "9 ['\\nSolar energy systems, net', '\\n\\xa0', '\\n\\xa0', '\\n6,106', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n6,138', '\\n\\xa0']\n",
      "9 ['\\nProperty, plant and equipment, net', '\\n\\xa0', '\\n\\xa0', '\\n10,638', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n10,396', '\\n\\xa0']\n",
      "9 ['\\nOperating lease right-of-use assets', '\\n\\xa0', '\\n\\xa0', '\\n1,197', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n1,218', '\\n\\xa0']\n",
      "9 ['\\nIntangible assets, net', '\\n\\xa0', '\\n\\xa0', '\\n323', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n339', '\\n\\xa0']\n",
      "9 ['\\nGoodwill', '\\n\\xa0', '\\n\\xa0', '\\n193', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n198', '\\n\\xa0']\n",
      "9 ['\\nOther non-current assets', '\\n\\xa0', '\\n\\xa0', '\\n1,373', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n1,470', '\\n\\xa0']\n",
      "9 ['\\nTotal assets', '\\n\\xa0', '\\n$', '\\n37,250', '\\n\\xa0', '\\n\\xa0', '\\n$', '\\n34,309', '\\n\\xa0']\n",
      "9 ['\\nLiabilities', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 ['\\nCurrent liabilities', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 ['\\nAccounts payable', '\\n\\xa0', '\\n$', '\\n3,970', '\\n\\xa0', '\\n\\xa0', '\\n$', '\\n3,771', '\\n\\xa0']\n",
      "9 ['\\nAccrued liabilities and other', '\\n\\xa0', '\\n\\xa0', '\\n2,825', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n3,222', '\\n\\xa0']\n",
      "9 ['\\nDeferred revenue', '\\n\\xa0', '\\n\\xa0', '\\n1,186', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n1,163', '\\n\\xa0']\n",
      "9 ['\\nCustomer deposits', '\\n\\xa0', '\\n\\xa0', '\\n788', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n726', '\\n\\xa0']\n",
      "9 ['\\nCurrent portion of debt and finance leases', '\\n\\xa0', '\\n\\xa0', '\\n3,217', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n1,785', '\\n\\xa0']\n",
      "9 ['\\nTotal current liabilities', '\\n\\xa0', '\\n\\xa0', '\\n11,986', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n10,667', '\\n\\xa0']\n",
      "9 ['\\nDebt and finance leases, net of current portion', '\\n\\xa0', '\\n\\xa0', '\\n10,666', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n11,634', '\\n\\xa0']\n",
      "9 ['\\nDeferred revenue, net of current portion', '\\n\\xa0', '\\n\\xa0', '\\n1,199', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n1,207', '\\n\\xa0']\n",
      "9 ['\\nOther long-term liabilities', '\\n\\xa0', '\\n\\xa0', '\\n2,667', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n2,691', '\\n\\xa0']\n",
      "9 ['\\nTotal liabilities', '\\n\\xa0', '\\n\\xa0', '\\n26,518', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n26,199', '\\n\\xa0']\n",
      "9 ['\\nCommitments and contingencies (Note 12)', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 ['\\nRedeemable noncontrolling interests in subsidiaries', '\\n\\xa0', '\\n\\xa0', '\\n632', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n643', '\\n\\xa0']\n",
      "9 ['\\nConvertible senior notes (Note 10)', '\\n\\xa0', '\\n\\xa0', '\\n60', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n—', '\\n\\xa0']\n",
      "9 ['\\nEquity', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 [\"\\nStockholders' equity\", '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0']\n",
      "9 ['\\nPreferred stock; $0.001 par value; 100 shares authorized;\\n\\xa0\\xa0 no shares issued and outstanding', '\\n\\xa0', '\\n\\xa0', '\\n—', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n—', '\\n\\xa0']\n",
      "9 ['\\nCommon stock; $0.001 par value; 2,000 shares authorized; 185 and\\n\\xa0\\xa0 181 shares issued and outstanding as of March 31, 2020 and December 31,\\n\\xa0\\xa0 2019, respectively', '\\n\\xa0', '\\n\\xa0', '\\n0', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n0', '\\n\\xa0']\n",
      "9 ['\\nAdditional paid-in capital', '\\n\\xa0', '\\n\\xa0', '\\n15,390', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n12,737', '\\n\\xa0']\n",
      "9 ['\\nAccumulated other comprehensive loss', '\\n\\xa0', '\\n\\xa0', '\\n(113', '\\n)', '\\n\\xa0', '\\n\\xa0', '\\n(36', '\\n)']\n",
      "9 ['\\nAccumulated deficit', '\\n\\xa0', '\\n\\xa0', '\\n(6,104', '\\n)', '\\n\\xa0', '\\n\\xa0', '\\n(6,083', '\\n)']\n",
      "9 [\"\\nTotal stockholders' equity\", '\\n\\xa0', '\\n\\xa0', '\\n9,173', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n6,618', '\\n\\xa0']\n",
      "9 ['\\nNoncontrolling interests in subsidiaries', '\\n\\xa0', '\\n\\xa0', '\\n867', '\\n\\xa0', '\\n\\xa0', '\\n\\xa0', '\\n849', '\\n\\xa0']\n",
      "9 ['\\nTotal liabilities and equity', '\\n\\xa0', '\\n$', '\\n37,250', '\\n\\xa0', '\\n\\xa0', '\\n$', '\\n34,309', '\\n\\xa0']\n"
     ]
    }
   ],
   "source": [
    "string_test = master_filings_dict['0001564590-20-019931']['filing_documents']['10-Q']['pages_code']\n",
    "test_df = table(string_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files Locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in QorK.items():\n",
    "    \n",
    "    #if k == '10-Q' and v == '0001564590-17-009968':\n",
    "        \n",
    "        for i, vv in enumerate(v):\n",
    "\n",
    "            # first grab 10-Q documents only\n",
    "            pages = master_filings_dict[v[i]]['filing_documents'][k]['pages_code']\n",
    "\n",
    "            try:\n",
    "                # Extract Table from html code and wrap it as a list of df.\n",
    "                table_df = table(pages)                 \n",
    "\n",
    "            except: \n",
    "                print('Something went wrong in file {}, file type = {}'.format(v[i], k))\n",
    "\n",
    "            with open('{}-{}.csv'.format(v[i], k), \"wb\") as fp:   # Pickling\n",
    "                pickle.dump(table_df, fp)\n",
    "\n",
    "            # display a status to the user.\n",
    "            print('All the pages from {} document {} have been tableized and saved.'.format(k, v[i]))\n",
    "            print('-'*80)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Old and the New."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: Older filing style. \n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 100)\n",
    "with open(\"0001564590-19-038256-10-Q.csv\", \"rb\") as fp:\n",
    "    file = pickle.load(fp)\n",
    "df01 = file[0]\n",
    "\n",
    "# df01['CSN_SUM'] = df01['Convertible Senior Notes'].values.sum()\n",
    "# del df01['Convertible Senior Notes']\n",
    "\n",
    "# df01['DeferredRevenue'] = df01['Deferred Revenue'].values.sum()\n",
    "# del df01['Deferred Revenue']\n",
    "\n",
    "# df01['NoncontrollingInterests'] = df01['Noncontrolling interests in subsidiaries'].values.sum()\n",
    "# del df01['Noncontrolling interests in subsidiaries']\n",
    "\n",
    "print(df01.shape)\n",
    "df01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_columns( df01 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newer Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: Newer filing style. \n",
    "import pickle \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "with open(\"0001564590-20-019931-10-Q.csv\", \"rb\") as fp:\n",
    "    file = pickle.load(fp)\n",
    "df02 = file[0]\n",
    "print(df02.shape) \n",
    "df02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_duplicate_columns( df02 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using the Function')\n",
    "\n",
    "duplicate_columns( df02 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slamming the Raw DataFrame into the dissected column cleanser. \n",
    "print('Not Using the Function')\n",
    "duplicates = [k for (k,v) in Counter(df02.columns).items() if v > 1] \n",
    "\n",
    "for dupli in duplicates: \n",
    "    res = df02[dupli].values.sum()\n",
    "    df02['Total {}'.format(dupli)] = res\n",
    "    df02.drop(dupli, axis=1, inplace=True)\n",
    "    \n",
    "df02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df02.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate col_name, add column tgt.\n",
    "df2 = df02.copy()\n",
    "\n",
    "# df2['CSN_SUM'] = df2['Convertible Senior Notes'].values.sum()\n",
    "# del df2['Convertible Senior Notes']\n",
    "\n",
    "df2['DeferredRevenue'] = df2['Deferred Revenue'].values.sum()\n",
    "del df2['Deferred Revenue']\n",
    "\n",
    "df2['NoncontrollingInterests'] = df2['Noncontrolling interests in subsidiaries'].values.sum()\n",
    "del df2['Noncontrolling interests in subsidiaries']\n",
    "\n",
    "#df2.set_index(['Year', 'Quarter'], inplace=True)\n",
    "\n",
    "print( df2.shape )\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_duplicate_columns( df2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ = pd.concat([df2, df01], axis=0, sort=False)\n",
    "\n",
    "print(c_.shape)\n",
    "c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serious Stuff. \n",
    "def duplicate_columns( DataF ):\n",
    "    \n",
    "    duplicates = [k for (k,v) in Counter(DataF.columns).items() if v > 1] \n",
    "\n",
    "    for dupli in duplicates: \n",
    "        \n",
    "        res = DataF[dupli].values.sum()\n",
    "        \n",
    "        DataF['Serious {}'.format(dupli)] = res\n",
    "        \n",
    "        DataF.drop(dupli, axis=1, inplace=True)\n",
    "\n",
    "    return DataF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for checking.\n",
    "def checking_duplicate_columns( DataF ):\n",
    "    \n",
    "    duplicates = [k for (k,v) in Counter(DataF.columns).items() if v > 1] \n",
    "\n",
    "    \n",
    "    print('Shape of df: ', DataF.shape )\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_.set_index(['Year', 'Quarter'], inplace=True)\n",
    "print(c_.shape)\n",
    "c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c1 = c_.copy()\n",
    "c1.dropna( axis= 1, inplace=True)\n",
    "print(c1.shape)\n",
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "QorK = {\n",
    "    '10-Q': ['0001564590-20-019931', '0001564590-19-038256', '0001564590-19-026445', '0001564590-19-013462', '0001564590-18-026353', '0001564590-18-019254', '0001564590-18-011086', '0001564590-17-021343', '0001564590-17-015705', '0001564590-17-009968', '0001564590-16-026820', '0001564590-16-023024', '0001564590-16-018886', '0001564590-15-009741', '0001564590-15-006666', '0001564590-15-003789'],\n",
    "    '10-K': ['0001564590-20-004475', '0001564590-19-003165', '0001564590-18-002956', '0001564590-17-003118', '0001564590-16-013195', '0001564590-15-001031']}\n",
    "QorK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating all Balance Sheet together.\n",
    "\n",
    "for k, v in QorK.items():\n",
    "    \n",
    "    #if k == '10-Q' and v == '0001564590-17-009968':\n",
    "        \n",
    "        for i, vv in enumerate(v):\n",
    "\n",
    "            # first grab 10-Q documents only\n",
    "            pages = master_filings_dict[v[i]]['filing_documents'][k]['pages_code']\n",
    "\n",
    "            try:\n",
    "                # Extract Table from html code and wrap it as a list of df.\n",
    "                table_df = table(pages)                 \n",
    "\n",
    "            except: \n",
    "                print('Something went wrong in file {}, file type = {}'.format(v[i], k))\n",
    "\n",
    "            with open('{}-{}.csv'.format(v[i], k), \"wb\") as fp:   # Pickling\n",
    "                pickle.dump(table_df, fp)\n",
    "                \n",
    "                  \n",
    "pd.set_option('display.max_columns', 100)\n",
    "with open(\"0001564590-15-001031-10-K.csv\", \"rb\") as fp:\n",
    "    file = pickle.load(fp)\n",
    "df1 = file[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statement Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS10_K(df)\n",
    "    \n",
    "    missing_values = [ '\\n', None, '\\n—', '\\n0', '\\n$', '\\n()']\n",
    "    \n",
    "    # Clean df.\n",
    "    df = df.drop([1,4,5,6,7,8], axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF10_K(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoO10_K(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS10_Q(table):\n",
    "    \n",
    "    BSdf = []\n",
    "    missing_values = [ '\\n', None, '\\n—', '\\n0', '\\n$', '\\n()']\n",
    "    \n",
    "    # Clean the table. \n",
    "    \n",
    "    row = table.replace(missing_values, '', regex=True)\n",
    "    file = file.drop(axis=1, columns = [1,4:], inplace=True)\n",
    "\n",
    "    statement = BSddf.DataFrame(row)\n",
    "    BSdf.append(statement)\n",
    "    \n",
    "    return BSdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF10_Q(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_table = []\n",
    "for table_num, table in enumerate(file):\n",
    "    \n",
    "    if table_num > 1:\n",
    "        print(table_num)\n",
    "        cleaned_row = cleaning_row(table)\n",
    "\n",
    "        res_table.append(cleaned_row)\n",
    "\n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Statment specific files for \n",
    "TSLA_BS10_K = []\n",
    "TSLA_CF10_K = []\n",
    "TSLA_SoO10_K = []\n",
    "TSLA_BS10_Q = []\n",
    "TSLA_CF10_Q = []\n",
    "TSLA_SoO10_Q = []\n",
    "\n",
    "for k, v in QorK.items():\n",
    "    \n",
    "    if k == '10-K':\n",
    "        \n",
    "        for i in range(len(v)):\n",
    "            \n",
    "    if k == '10-Q'\n",
    "    \n",
    "        for i in range(len(v)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/50950614/converting-column-into-multi-index-column"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
