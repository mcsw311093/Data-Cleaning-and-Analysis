{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "\n",
    "AN_Active = {'0001564590-20-019931': ['#Consolidated_Balance_Sheets',\n",
    "  'https://www.sec.gov/Archives/edgar/data/0001318605/000156459020019931/0001564590-20-019931.txt']}\n",
    "\n",
    "# 2016 onwards. Use the above Accession_Number_URL_16_20, 2016+ need to use .txt format. \n",
    "for AN, key_url in AN_Active.items():\n",
    "    year = int( AN.split( sep='-' )[1] )\n",
    "    # Break is needed to limit lower bound, either \n",
    "    # pre-2016 = 0001193125-12-170665 (if excludes -12-170665 ) or \n",
    "    # post-16 = 0001193125-16-742796 (x > 16, excludes 16)\n",
    "    print('~'*75)\n",
    "\n",
    "    # if statement takes in Acc-Num: xx-xxx-xx\n",
    "    if AN == '0001193125-12-170665':\n",
    "        break\n",
    "\n",
    "    if year < 21:\n",
    "        \n",
    "        print(f'Accession Number - {AN}\\n')\n",
    "        key = key_url[0]\n",
    "        print(f'key - {key_url[0]}\\n')\n",
    "        act_url = key_url[1]\n",
    "        print(f'Active URL - {key_url[1]}\\n')\n",
    "        \n",
    "        r = requests.get(act_url)\n",
    "        soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "        filing_document = soup.find('document')\n",
    "#         print('filing_document - ', filing_document)\n",
    "        \n",
    "        document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "        print(f'document_id - {document_id}\\n' )\n",
    "\n",
    "        page_key = soup.find_all(attrs={\"id\": \"{}\".format(key.replace('#', '') ) } )\n",
    "        print(f'page_key - {page_key}\\n' )\n",
    "        \n",
    "        T_table = [ table.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling for table in page_key ]\n",
    "#         print(f'T_table - {T_table}')\n",
    "\n",
    "        target = T_table[0].findAll('tr')\n",
    "#         print(f'target - {target[0]}\\n')\n",
    "        row_list = [ row for row in target if row.get_text(strip=True, separator=',') != '']\n",
    "#         print(f'row_list - {row_list[0]}\\n')\n",
    "        col_list = []\n",
    "\n",
    "        for row in row_list:\n",
    "            col = row.find_all('td')\n",
    "#             print(f'col - {col}')\n",
    "            column = []\n",
    "            for tr in col:\n",
    "                ele = tr.text\n",
    "#                 print(f'ele - {ele}')\n",
    "                try:\n",
    "                    ele= ele.encode('cp1252').decode(\"utf-8\", \"ignore\")\n",
    "                except:\n",
    "                    ele.encode('latin1').decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "                column.append(ele)\n",
    "#             print(f'column - {column}')\n",
    "            col_list.append(column)\n",
    "\n",
    "#         col_list.pop(0)\n",
    "\n",
    "        new_col_list = []\n",
    "        \n",
    "        # Tesla Date is split to Q[0] and Y[1].\n",
    "        col_list[0] = col_list[0] + col_list[1]\n",
    "        col_list.pop(1)\n",
    "        \n",
    "        for col_num, column in enumerate(col_list): \n",
    "\n",
    "            if col_num == 0:\n",
    "#                 print('raw column - ', column)\n",
    "                Q, Y = Quarter(column)\n",
    "#                 print(Q, Y)\n",
    "                new_col_list.append(Q)\n",
    "                new_col_list.append(Y)\n",
    "\n",
    "            else:\n",
    "                column.pop(1)\n",
    "                column.pop(1)\n",
    "#                 print(f'column - {column}')\n",
    "                new_col_list.append(cleaning_column( column ))\n",
    "\n",
    "        df = pd.DataFrame(new_col_list)\n",
    "        df = df.T.reset_index(drop=True)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.drop([row for row in range(df.shape[0]) if row != 1], axis=0).reset_index(drop=True)\n",
    "        ICD.display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my capstone project I want to try to use machine learning to aggregate information from different financial analytical approaches \n",
    "\n",
    "Tesla's disruption to the automotive industry began back in 2008 with its exclusive highend Roadster () up until the recent reveal of the mainstream Model Y ().\n",
    "\n",
    "As the pioneer and leader in EV sales worldwide (), Tesla perhaps has gained more attention in its financial sustainability (); as of this writing, Tesla has yet produced a profittable quarter(). This ***uncertainity of unprofitablity*** has resonated throughout the EV market and automotive market in large, which in turn made Tesla to become one of the most important benchmark indicator of the EV story (). \n",
    "\n",
    "~~~~~\n",
    "\n",
    "This study aims to explore whether Tesla's business model is more related to traditional automakers or more closely related to tech companies. We will use a multi-class classification \n",
    "\n",
    "In this report I will attempt to predict the Tesla's stock price using different financial and statistical models. In the end I will discuss the performance of each model and some future recommendations. \n",
    "\n",
    "~~~~~\n",
    "\n",
    "### **Investment Horizon of Models** \n",
    "\n",
    "1) **Intrinsic Value** Long Term (Quarters/Years)\n",
    "\n",
    "2) **Discounted Cash Flow** Mid Term (Quarters)\n",
    "\n",
    "3) **Comparaable Analysis** Mid Term (Days/Months)\n",
    "\n",
    "4) **Time Series** Short (Days)\n",
    "\n",
    "    - Time Autoregressive Integrated Moving Average (ARIMA)\n",
    "     https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3168423\n",
    "\n",
    "Model Evalution\n",
    "AUROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library \n",
    "\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd \n",
    "import csv\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Text Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "    Replace C1 control characters in the Unicode string s by the\n",
    "    characters at the corresponding code points in Windows-1252,\n",
    "    where possible.\n",
    "    \"\"\"\n",
    "\n",
    "def to_windows_1252(match):\n",
    "    try:\n",
    "        return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "    except UnicodeDecodeError:\n",
    "        # No character at the corresponding code point: remove it.\n",
    "        return ''\n",
    "\n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the Document Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_doc_content( brand, CIK ):\n",
    "    \n",
    "    company = {}\n",
    "    \n",
    "    company['auto'] = {}\n",
    "    auto = {brand : CIK\n",
    "           }\n",
    "    company['auto'] = auto \n",
    "\n",
    "    key = list(auto.copy().values())\n",
    "    \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0001318605']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grab_doc_content('Tesla', '0001318605')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_filings_dict = {}\n",
    "    \n",
    "file_code = {}\n",
    "    \n",
    "file_text = {}\n",
    "\n",
    "Accession_Number_URL = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Directory For CIK    \n",
    "dir_url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-&dateb=&owner=include&count=100'\n",
    "dir_url_list = [dir_url.format(x) for x in grab_doc_content('Tesla', '0001318605')]\n",
    "\n",
    "#print('Directory URL: {}'.format(dir_url_list))\n",
    "doc_url_list = [] \n",
    "\n",
    "# FOR-loop yielding Accession Numbers from CIK/URL Directory.\n",
    "for CIK_num in grab_doc_content('Tesla', '0001318605'):\n",
    "\n",
    "    doc_url = r'https://www.sec.gov/Archives/edgar/data/{CIKx}/{xx}/{yy}.txt'\n",
    "    doc_url_new = doc_url.format(CIKx = CIK_num, xx='{xx}', yy ='{yy}')\n",
    "    doc_url_list.append(doc_url_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_maker(dir_url):\n",
    "    dir_url_list = [dir_url.format(x) for x in grab_doc_content('Tesla', '0001318605')]\n",
    "\n",
    "    #print('Directory URL: {}'.format(dir_url_list))\n",
    "    doc_url_list = [] \n",
    "\n",
    "    # FOR-loop yielding Accession Numbers from CIK/URL Directory.\n",
    "    for CIK_num in grab_doc_content('Tesla', '0001318605'):\n",
    "\n",
    "        doc_url = r'https://www.sec.gov/Archives/edgar/data/{CIKx}/{xx}/{yy}.txt'\n",
    "        doc_url_new = doc_url.format(CIKx = CIK_num, xx='{xx}', yy ='{yy}')\n",
    "        \n",
    "        doc_url_list.append(doc_url_new)\n",
    "        \n",
    "        url_lists = zip( dir_url_list , doc_url_list )\n",
    "        \n",
    "    return url_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001318605&type=10-&dateb=&owner=include&count=100\n",
      "2 https://www.sec.gov/Archives/edgar/data/0001318605/{xx}/{yy}.txt\n"
     ]
    }
   ],
   "source": [
    "url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-&dateb=&owner=include&count=100'\n",
    "for dir_url, doc_url in url_maker(url): \n",
    "    print(1, dir_url)\n",
    "    print(2, doc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accession_Number_URL = {}\n",
    "url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-&dateb=&owner=include&count=100'\n",
    "\n",
    "for dir_url, doc_url in url_maker(url):\n",
    "    \n",
    "    response = requests.get(dir_url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    text = soup.get_text(strip=True)\n",
    "\n",
    "    cleaned_text = re.findall('Acc-no: \\d+-\\d+-\\d+' , text)\n",
    "\n",
    "    accession_number = [n.replace('Acc-no: ', '') for n in cleaned_text]\n",
    "    accessionnumber = [num.replace('-', '') for num in accession_number]\n",
    "\n",
    "    accession_numbers = zip(accessionnumber, accession_number)\n",
    "    \n",
    "    cikk = [ cikk_.replace('CIK=', '') for cikk_ in re.findall('CIK=\\d+', dir_url) ][0]\n",
    "    CIKK = {cikk : accessionnumber}\n",
    "    \n",
    "    for (a,b) in accession_numbers: \n",
    "        master_filings_dict[b] = {}\n",
    "        master_filings_dict[b]['sec_header_content'] = {}\n",
    "        master_filings_dict[b]['filing_documents'] = None\n",
    "\n",
    "        doc_url_single = doc_url.format(xx = a, yy = b)\n",
    "\n",
    "        file_url_list = []\n",
    "\n",
    "        \n",
    "\n",
    "        file_url_list.append( doc_url_single )\n",
    "        \n",
    "        Accession_Number_URL.update({ b : file_url_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001564590-20-019931', '0001564590-20-018984', '0001564590-20-004475', '0001564590-19-038256', '0001564590-19-026445', '0001564590-19-013462', '0001564590-19-003165', '0001564590-18-026353', '0001564590-18-019254', '0001564590-18-011086', '0001564590-18-002956', '0001564590-17-021343', '0001564590-17-015705', '0001564590-17-009968', '0001564590-17-003118', '0001564590-16-026820', '0001564590-16-023024', '0001564590-16-018886', '0001564590-16-013195', '0001564590-15-009741', '0001564590-15-006666', '0001564590-15-003789', '0001564590-15-001031']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "A_N = []\n",
    "for AN, url in Accession_Number_URL.items():\n",
    "    \n",
    "    if AN == '0001193125-14-403635':\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        A_N.append(AN)\n",
    "print(A_N)\n",
    "print(len(A_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each Filing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 71 page(s) found.\n",
      "There was 71 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-20-019931 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K/A was parsed.\n",
      "There was 67 page(s) found.\n",
      "There was 67 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-20-018984 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 158 page(s) found.\n",
      "There was 158 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-20-004475 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 72 page(s) found.\n",
      "There was 72 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-19-038256 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 70 page(s) found.\n",
      "There was 70 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-19-026445 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 67 page(s) found.\n",
      "There was 67 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-19-013462 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 175 page(s) found.\n",
      "There was 175 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-19-003165 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 71 page(s) found.\n",
      "There was 71 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-18-026353 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 64 page(s) found.\n",
      "There was 64 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-18-019254 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 61 page(s) found.\n",
      "There was 61 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-18-011086 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 154 page(s) found.\n",
      "There was 154 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-18-002956 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 60 page(s) found.\n",
      "There was 60 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-17-021343 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 58 page(s) found.\n",
      "There was 58 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-17-015705 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 58 page(s) found.\n",
      "There was 58 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-17-009968 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 143 page(s) found.\n",
      "There was 143 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-17-003118 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 55 page(s) found.\n",
      "There was 55 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-16-026820 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 50 page(s) found.\n",
      "There was 50 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-16-023024 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 50 page(s) found.\n",
      "There was 50 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-16-018886 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 81 page(s) found.\n",
      "There was 81 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-16-013195 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 51 page(s) found.\n",
      "There was 51 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-15-009741 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 49 page(s) found.\n",
      "There was 49 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-15-006666 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-Q was parsed.\n",
      "There was 52 page(s) found.\n",
      "There was 52 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-15-003789 were parsed and stored.\n",
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 97 page(s) found.\n",
      "There was 97 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001564590-15-001031 were parsed and stored.\n"
     ]
    }
   ],
   "source": [
    "QorK = {'10-Q' : [],\n",
    "        '10-K' : []\n",
    "       }\n",
    "\n",
    "for acc_num, url in Accession_Number_URL.items():\n",
    "    \n",
    "    master_document_dict = {}\n",
    "    \n",
    "    # create a stop point\n",
    "    if acc_num == '0001193125-14-403635':\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # grab the response\n",
    "        response = requests.get(url[0])\n",
    "\n",
    "    # Soupify\n",
    "        # pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        filing_document = soup.find('document')\n",
    "        \n",
    "    # Parsing\n",
    "        # Document type ->> document_id\n",
    "        document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "        # Document sequence ->> document_sequence\n",
    "        document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "        # Document filename ->> document_filename\n",
    "        document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "        # Document description ->> document_description\n",
    "        document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "        \n",
    "    # Storage    \n",
    "        # initalize our document dictionary\n",
    "        master_document_dict[document_id] = {}\n",
    "\n",
    "        # add the different parts, we parsed up above.\n",
    "        master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "        master_document_dict[document_id]['document_filename'] = document_filename\n",
    "        master_document_dict[document_id]['document_description'] = document_description\n",
    "        \n",
    "# Scraping\n",
    "        # grab the text portion of the document, this will be used to split the document into pages.\n",
    "        filing_doc_text = filing_document.find('text').extract()\n",
    "\n",
    "        # find all the thematic breaks, these help define page numbers and page breaks.\n",
    "        all_thematic_breaks = filing_doc_text.find_all('hr')\n",
    "\n",
    "        # Locate and store page number via list comprehension.\n",
    "        all_page_numbers = [thematic_break.previous_sibling.previous_sibling.get_text(strip=True) for thematic_break in all_thematic_breaks]\n",
    "\n",
    "        # convert all thematic breaks to a string so it can be used for parsing\n",
    "        all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "\n",
    "        # prep the document text for splitting, this means converting it to a string.\n",
    "        filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "        # handle the case where there are thematic breaks.\n",
    "        if len(all_thematic_breaks) > 0: \n",
    "\n",
    "            # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "            regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "\n",
    "            # split the document along each thematic break.\n",
    "            split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "            # store the document itself\n",
    "            master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "\n",
    "        # handle the case where there are no thematic breaks.\n",
    "        elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "            # handles so it will display correctly.\n",
    "            split_filing_string = all_thematic_breaks\n",
    "\n",
    "            # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "            master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "            \n",
    "        # display some information to the user.\n",
    "        print('-'*80)\n",
    "        print('The document {} was parsed.'.format(document_id))\n",
    "        print('There was {} page(s) found.'.format(len(all_page_numbers)))\n",
    "        print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "\n",
    "        # store the documents in the master_filing_dictionary.\n",
    "        master_filings_dict[acc_num]['filing_documents'] = master_document_dict\n",
    "        \n",
    "        # if document is 10-Q\n",
    "        if document_id == '10-Q':\n",
    "            QorK['10-Q'].append(acc_num) # add acc_num to QorK in 10-Q as key\n",
    "        \n",
    "        # if document is 10-K\n",
    "        if document_id == '10-K':\n",
    "            QorK['10-K'].append(acc_num) # add acc_num to QorK in 10-K as key\n",
    "        \n",
    "        del master_document_dict\n",
    "        \n",
    "        print('-'*80)\n",
    "        print('All the documents for filing {} were parsed and stored.'.format(acc_num))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_test = master_filings_dict['0001564590-20-019931']['filing_documents']['10-Q']['pages_code']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Check Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variations of HTML anchors.\n",
    "CBS_A = '<a name=\"Consolidated_Balance_Sheets'\n",
    "CBS_B = 'id=\"CONSOLIDATED_BALANCE_SHEETS'\n",
    "CBS3 = 'name=\"CONSOLIDATED_BALANCE_SHEETS\"' \n",
    "CBS10Knew = 'id=\"Consolidated_Balance_Sheets\"'\n",
    "\n",
    "SoO_A = '<a name=\"Statements_of_Operations\"' \n",
    "SoO_B = 'id=\"CONSOLIDATED_STATEMENTS_OPERATIONS\"'\n",
    "SoO3 = 'name=\"CONSOLIDATED_STATEMENTS_OPERATIONS\"'\n",
    "CSoO10K = 'name=\"Consolidated_Statements_of_Operations\"'\n",
    "CSoO10Knew = 'id=\"Consolidated_Statements_of_Operations\"'\n",
    "\n",
    "CF_A = '<a name=\"Statements_of_Cash'\n",
    "CF_B = 'id=\"CONSOLIDATED_STATEMENTS_CASH_FLOWS\"'\n",
    "CF3 = 'name=\"CONSOLIDATED_STATEMENTS_CASH_FLOWS\"'\n",
    "CCF10K = 'name=\"Consolidated_Statements_of_Cash_Flows\"'\n",
    "CCF10Knew = 'id=\"Consolidated_Statements_of_Cash_Flows\"'\n",
    "\n",
    "def anchor_check(page_code):\n",
    "    \n",
    "    # if the page has one of these anchors\n",
    "    if (CBS_A in page_code) or (CBS_B in page_code) or (CBS3 in page_code) or (CBS10Knew in page_code):\n",
    "        return True\n",
    "\n",
    "    #elif (SoO_A in page_code) or (SoO_B in page_code) or (SoO3 in page_code) or (CSoO10K in page_code) or (CSoO10Knew in page_code) :\n",
    "        #return True\n",
    "\n",
    "    #elif (CF_A in page_code) or (CF_B in page_code) or (CF3 in page_code) or (CCF10K in page_code) or (CCF10Knew in page_code):\n",
    "        #return True\n",
    "\n",
    "\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving string test to local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're looking at the following file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string_test = master_filings_dict['0001564590-20-019931']['filing_documents']['10-Q']['pages_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stringtest.csv\", \"wb\") as fp:\n",
    "    pickle.dump(string_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading string test from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"stringtest.csv\", \"rb\") as fp:\n",
    "    string_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Columns Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dict = {'December' : 'Q4',\n",
    "              'November' : 'Q4',\n",
    "             'October' : 'Q4',\n",
    "             'September' : 'Q3',\n",
    "             'August': 'Q3',\n",
    "             'July' : 'Q3',\n",
    "             'June' : 'Q2',\n",
    "             'May' : 'Q2',\n",
    "             'April' : 'Q2',\n",
    "             'March' : 'Q1',\n",
    "             'Feburary' : 'Q1',\n",
    "             'January' : 'Q1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['Quarter', 'Year', 'Assets', 'Current assets',\n",
    "       'Cash and cash equivalents',\n",
    "       'Restricted cash and marketable securities', 'Accounts receivable',\n",
    "       'Inventory', 'Prepaid expenses and other current assets',\n",
    "       'Total current assets', 'Operating lease vehicles, net',\n",
    "       'Property, plant and equipment, net', 'Restricted cash', 'Other assets',\n",
    "       'Total assets',\n",
    "       'Current liabilities', 'Accounts payable', 'Accrued Liabilities',\n",
    "       'Deferred Revenue into ', 'Capital lease obligations, current portion',\n",
    "       'Customer deposits', 'Convertible Senior Notes (Note 8 )',\n",
    "       'Total current liabilities',\n",
    "       'Capital lease obligations, less current portion', 'Deferred Revenue',\n",
    "       'Convertible Senior Notes', 'Resale value guarantee',\n",
    "       'Other long-term liabilities', 'Total liabilities',\n",
    "       'Commitments and contingencies (Note 11)', 'Convertible Senior Notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_conversion_dict = {\n",
    "    \n",
    "}\n",
    "def col_name_check( column_name ):\n",
    "    \n",
    "    for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortening Column name.\n",
    "def clean_col_name( element ):\n",
    "    \n",
    "    if re.findall('Common stock.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Common Stock') \n",
    "\n",
    "        return element\n",
    "        \n",
    "    elif re.findall('Preferred stock.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Preferred Stock') \n",
    "        \n",
    "        return element\n",
    "    \n",
    "    elif re.findall(\"Total liabilities and stockholders' equity.*\", element):\n",
    "        \n",
    "        element = element.replace(element, 'Total liabilities and equity' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Accounts receivable.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Accounts receivable' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Accrued liabilities.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Accrued Liabilities' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Deferred revenue.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Deferred Revenue' ) \n",
    "        \n",
    "        return element\n",
    "\n",
    "    elif re.findall('Redeemable noncontrolling interests in subsidiaries.*', element):\n",
    "        \n",
    "        element = element.replace(element, 'Noncontrolling interests in subsidiaries' ) \n",
    "        \n",
    "        return element\n",
    "    \n",
    "    elif re.findall('Convertible senior notes.*', element, flags=re.I):\n",
    "        #print('before: ', element)\n",
    "        element = element.replace(element, 'Convertible Senior Notes' )\n",
    "        #print('after: ', element)\n",
    "        return element\n",
    "    \n",
    "    else:\n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw column list-like object into cleaning function. \n",
    "def cleaning_column(column, m):\n",
    "     \n",
    "    column_list = []\n",
    "\n",
    "    for element_post, element in enumerate(column):\n",
    "        \n",
    "        # Handling the first \"row\" in the table i.e. column name.\n",
    "        if element_post == 0:\n",
    "\n",
    "            element = unicodedata.normalize('NFKD', element)\n",
    "            \n",
    "            element = element.replace( '\\n', '')\n",
    "            \n",
    "            clean_ele = clean_col_name( element )\n",
    "            \n",
    "            column_list.append(clean_ele)\n",
    "            \n",
    "        # Use a dictionary to convert the Months into numbers.\n",
    "        #elif element_post == :\n",
    "            #month = re.sub('({})'.format('|'.join(map(re.escape, month_dict.keys()))), lambda m: month_dict[m.group()], dat)\n",
    "            #column_list.extend(month)\n",
    "        #elif element == '(' or ')': \n",
    "            #pint(element)\n",
    "            #continue\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            element = element.replace( '\\n$' and '\\n\\xa0' and '\\n', '0')\n",
    "            \n",
    "            element = element.replace( ',', '' )\n",
    "\n",
    "            pattern = re.compile(r'[\\d]+,[\\d]+|[\\d]+')\n",
    "\n",
    "            res = pattern.findall(element)\n",
    "\n",
    "            res = [int(ele) * m for ele in res]\n",
    "\n",
    "            column_list.extend(res)\n",
    "\n",
    "    return column_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tables Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date( column ):\n",
    "    \n",
    "    date = []\n",
    "    \n",
    "    for ele in column:\n",
    "        \n",
    "        norm_ele = unicodedata.normalize('NFKD', ele)\n",
    "        \n",
    "        date_data = re.sub('({})'.format('|'.join(map(re.escape, month_dict.keys()))), lambda m: month_dict[m.group()], norm_ele)\n",
    "\n",
    "        element = date_data.replace( '\\n' and ' ' and '\\n ', '0')\n",
    "        \n",
    "        pattern = re.compile(r'\\w+')\n",
    "\n",
    "        res = pattern.findall(element)\n",
    "        \n",
    "        res = str(res[0]) \n",
    "\n",
    "        date.append(res)\n",
    "\n",
    "    date.pop(-1)\n",
    "\n",
    "    date.insert(0, 'Quarter')\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year( column ):\n",
    "    \n",
    "    years = []\n",
    "                \n",
    "    for year in column:\n",
    "\n",
    "        norm_year = unicodedata.normalize('NFKD', year)\n",
    "\n",
    "        clean_year = norm_year.replace( '\\n', '0')\n",
    "\n",
    "        years.append(str(int(clean_year)))\n",
    "        \n",
    "    years.pop(-1)\n",
    "\n",
    "    years.insert(0, 'Year')\n",
    "    \n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_extractor( page, multiplier ):\n",
    "    \n",
    "    # Convert string to BS object\n",
    "    soup = BeautifulSoup(page, 'html5')\n",
    "\n",
    "    # then get all the rows in the table.\n",
    "    table_rows = soup.find_all('tr')\n",
    "    \n",
    "    if len( [tr.text for tr in table_rows[0].find_all('td')] ) < 3:\n",
    "        del table_rows[0]\n",
    "    \n",
    "    single_table = []\n",
    "    \n",
    "    # Rotate through each column, adding in three structures to single_table.\n",
    "    for tr_post, tr in enumerate(table_rows):\n",
    "\n",
    "        td = tr.find_all('td')\n",
    "\n",
    "        column = [tr.text for tr in td]\n",
    "        \n",
    "        if tr_post == 0:\n",
    "            \n",
    "            col_date = date( column )\n",
    "\n",
    "            single_table.append( col_date )\n",
    "         \n",
    "        elif tr_post == 1: \n",
    "            \n",
    "            col_year = year( column )\n",
    "            \n",
    "            single_table.append( col_year )\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            cleaned_data = cleaning_column( column , multiplier )\n",
    "            \n",
    "            if len(cleaned_data) > 10:\n",
    "                del cleaned_data[3]\n",
    "            \n",
    "            single_table.append( cleaned_data )\n",
    "            \n",
    "    return single_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_clean( single_table ):\n",
    "    #print(single_table)\n",
    "    table_df = pd.DataFrame(single_table)\n",
    "\n",
    "    table_df = table_df.transpose()  \n",
    "\n",
    "    table_df.drop(table_df.index[1:3], 0, inplace=True)\n",
    "\n",
    "    table_df.drop(table_df.index[2:], 0, inplace=True)\n",
    "\n",
    "    table_df.columns = table_df.iloc[0]\n",
    "\n",
    "    table_df = table_df.drop(table_df.index[0])\n",
    "    \n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table(page_code):\n",
    "    \n",
    "    doc_tables=[]\n",
    "\n",
    "    for page_post, page in enumerate(page_code):\n",
    "        \n",
    "        if anchor_check(page) and ('in millions' in page):\n",
    "            \n",
    "            single_table = table_extractor( page, 10**6 )\n",
    "            \n",
    "            table_df = df_clean(single_table)\n",
    "                        \n",
    "            doc_tables.append(table_df)\n",
    "        \n",
    "        elif anchor_check(page) and ('in thousands' in page):\n",
    "            \n",
    "            single_table = table_extractor( page, 10**3 )\n",
    "            \n",
    "            table_df = df_clean(single_table)\n",
    "            \n",
    "            doc_tables.append(table_df)\n",
    "                \n",
    "    return doc_tables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Year</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current assets</th>\n",
       "      <th>Cash and cash equivalents</th>\n",
       "      <th>Restricted cash</th>\n",
       "      <th>Accounts receivable</th>\n",
       "      <th>Inventory</th>\n",
       "      <th>Prepaid expenses and other current assets</th>\n",
       "      <th>Total current assets</th>\n",
       "      <th>Operating lease vehicles, net</th>\n",
       "      <th>Solar energy systems, net</th>\n",
       "      <th>Property, plant and equipment, net</th>\n",
       "      <th>Operating lease right-of-use assets</th>\n",
       "      <th>Intangible assets, net</th>\n",
       "      <th>Goodwill</th>\n",
       "      <th>MyPower customer notes receivable, net of current portion</th>\n",
       "      <th>Restricted cash, net of current portion</th>\n",
       "      <th>Other assets</th>\n",
       "      <th>Total assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current liabilities</th>\n",
       "      <th>Accounts payable</th>\n",
       "      <th>Accrued Liabilities</th>\n",
       "      <th>Deferred Revenue</th>\n",
       "      <th>Resale value guarantees</th>\n",
       "      <th>Customer deposits</th>\n",
       "      <th>Current portion of debt and finance leases</th>\n",
       "      <th>Total current liabilities</th>\n",
       "      <th>Debt and finance leases, net of current portion</th>\n",
       "      <th>Deferred Revenue</th>\n",
       "      <th>Resale value guarantees, net of current portion</th>\n",
       "      <th>Other long-term liabilities</th>\n",
       "      <th>Total liabilities</th>\n",
       "      <th>Commitments and contingencies (Note 16)</th>\n",
       "      <th>Noncontrolling interests in subsidiaries</th>\n",
       "      <th>Equity</th>\n",
       "      <th>Stockholders' equity</th>\n",
       "      <th>Preferred Stock</th>\n",
       "      <th>Common Stock</th>\n",
       "      <th>Additional paid-in capital</th>\n",
       "      <th>Accumulated other comprehensive loss</th>\n",
       "      <th>Accumulated deficit</th>\n",
       "      <th>Total stockholders' equity</th>\n",
       "      <th>Noncontrolling interests in subsidiaries</th>\n",
       "      <th>Total liabilities and equity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Q4</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6268000000</td>\n",
       "      <td>246000000</td>\n",
       "      <td>1324000000</td>\n",
       "      <td>3552000000</td>\n",
       "      <td>713000000</td>\n",
       "      <td>12103000000</td>\n",
       "      <td>2447000000</td>\n",
       "      <td>6138000000</td>\n",
       "      <td>10396000000</td>\n",
       "      <td>1218000000</td>\n",
       "      <td>339000000</td>\n",
       "      <td>198000000</td>\n",
       "      <td>393000000</td>\n",
       "      <td>269000000</td>\n",
       "      <td>808000000</td>\n",
       "      <td>34309000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3771000000</td>\n",
       "      <td>2905000000</td>\n",
       "      <td>1163000000</td>\n",
       "      <td>317000000</td>\n",
       "      <td>726000000</td>\n",
       "      <td>1785000000</td>\n",
       "      <td>10667000000</td>\n",
       "      <td>11634000000</td>\n",
       "      <td>1207000000</td>\n",
       "      <td>36000000</td>\n",
       "      <td>2655000000</td>\n",
       "      <td>26199000000</td>\n",
       "      <td>0</td>\n",
       "      <td>643000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12737000000</td>\n",
       "      <td>36000000</td>\n",
       "      <td>6083000000</td>\n",
       "      <td>6618000000</td>\n",
       "      <td>849000000</td>\n",
       "      <td>34309000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 Quarter  Year Assets Current assets Cash and cash equivalents  \\\n",
       "3      Q4  2019      0              0                6268000000   \n",
       "\n",
       "0 Restricted cash Accounts receivable   Inventory  \\\n",
       "3       246000000          1324000000  3552000000   \n",
       "\n",
       "0 Prepaid expenses and other current assets Total current assets  \\\n",
       "3                                 713000000          12103000000   \n",
       "\n",
       "0 Operating lease vehicles, net Solar energy systems, net  \\\n",
       "3                    2447000000                6138000000   \n",
       "\n",
       "0 Property, plant and equipment, net Operating lease right-of-use assets  \\\n",
       "3                        10396000000                          1218000000   \n",
       "\n",
       "0 Intangible assets, net   Goodwill  \\\n",
       "3              339000000  198000000   \n",
       "\n",
       "0 MyPower customer notes receivable, net of current portion  \\\n",
       "3                                          393000000          \n",
       "\n",
       "0 Restricted cash, net of current portion Other assets Total assets  \\\n",
       "3                               269000000    808000000  34309000000   \n",
       "\n",
       "0 Liabilities Current liabilities Accounts payable Accrued Liabilities  \\\n",
       "3           0                   0       3771000000          2905000000   \n",
       "\n",
       "0 Deferred Revenue Resale value guarantees Customer deposits  \\\n",
       "3       1163000000               317000000         726000000   \n",
       "\n",
       "0 Current portion of debt and finance leases Total current liabilities  \\\n",
       "3                                 1785000000               10667000000   \n",
       "\n",
       "0 Debt and finance leases, net of current portion Deferred Revenue  \\\n",
       "3                                     11634000000       1207000000   \n",
       "\n",
       "0 Resale value guarantees, net of current portion Other long-term liabilities  \\\n",
       "3                                        36000000                  2655000000   \n",
       "\n",
       "0 Total liabilities Commitments and contingencies (Note 16)  \\\n",
       "3       26199000000                                       0   \n",
       "\n",
       "0 Noncontrolling interests in subsidiaries Equity Stockholders' equity  \\\n",
       "3                                643000000      0                    0   \n",
       "\n",
       "0 Preferred Stock Common Stock Additional paid-in capital  \\\n",
       "3               0            0                12737000000   \n",
       "\n",
       "0 Accumulated other comprehensive loss Accumulated deficit  \\\n",
       "3                             36000000          6083000000   \n",
       "\n",
       "0 Total stockholders' equity Noncontrolling interests in subsidiaries  \\\n",
       "3                 6618000000                                849000000   \n",
       "\n",
       "0 Total liabilities and equity  \n",
       "3                  34309000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_test = master_filings_dict['0001564590-20-004475']['filing_documents']['10-K']['pages_code']\n",
    "test_df = table(string_test)\n",
    "table_df = test_df[0]\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files Locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the pages from 10-Q document 0001564590-20-019931 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-19-038256 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-19-026445 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-19-013462 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-18-026353 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-18-019254 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-18-011086 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-17-021343 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-17-015705 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-17-009968 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-16-026820 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-16-023024 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-16-018886 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-15-009741 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-15-006666 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-Q document 0001564590-15-003789 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-K document 0001564590-20-004475 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-K document 0001564590-19-003165 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-K document 0001564590-18-002956 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-K document 0001564590-17-003118 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-K document 0001564590-16-013195 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from 10-K document 0001564590-15-001031 have been tableized and saved.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in QorK.items():\n",
    "    \n",
    "    #if k == '10-Q' and v == '0001564590-17-009968':\n",
    "        \n",
    "        for i, vv in enumerate(v):\n",
    "\n",
    "            # first grab 10-Q documents only\n",
    "            pages = master_filings_dict[v[i]]['filing_documents'][k]['pages_code']\n",
    "\n",
    "            try:\n",
    "                # Extract Table from html code and wrap it as a list of df.\n",
    "                table_df = table(pages)                 \n",
    "\n",
    "            except: \n",
    "                print('Something went wrong in file {}, file type = {}'.format(v[i], k))\n",
    "\n",
    "            with open('{}-{}.csv'.format(v[i], k), \"wb\") as fp:   # Pickling\n",
    "                pickle.dump(table_df, fp)\n",
    "\n",
    "            # display a status to the user.\n",
    "            print('All the pages from {} document {} have been tableized and saved.'.format(k, v[i]))\n",
    "            print('-'*80)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Old and the New."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: Older filing style. \n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 100)\n",
    "with open(\"0001564590-19-038256-10-Q.csv\", \"rb\") as fp:\n",
    "    file = pickle.load(fp)\n",
    "df01 = file[0]\n",
    "\n",
    "# df01['CSN_SUM'] = df01['Convertible Senior Notes'].values.sum()\n",
    "# del df01['Convertible Senior Notes']\n",
    "\n",
    "# df01['DeferredRevenue'] = df01['Deferred Revenue'].values.sum()\n",
    "# del df01['Deferred Revenue']\n",
    "\n",
    "# df01['NoncontrollingInterests'] = df01['Noncontrolling interests in subsidiaries'].values.sum()\n",
    "# del df01['Noncontrolling interests in subsidiaries']\n",
    "\n",
    "print(df01.shape)\n",
    "df01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_columns( df01 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newer Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: Newer filing style. \n",
    "import pickle \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "with open(\"0001564590-20-019931-10-Q.csv\", \"rb\") as fp:\n",
    "    file = pickle.load(fp)\n",
    "df02 = file[0]\n",
    "print(df02.shape) \n",
    "df02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_duplicate_columns( df02 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using the Function')\n",
    "\n",
    "duplicate_columns( df02 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slamming the Raw DataFrame into the dissected column cleanser. \n",
    "print('Not Using the Function')\n",
    "duplicates = [k for (k,v) in Counter(df02.columns).items() if v > 1] \n",
    "\n",
    "for dupli in duplicates: \n",
    "    res = df02[dupli].values.sum()\n",
    "    df02['Total {}'.format(dupli)] = res\n",
    "    df02.drop(dupli, axis=1, inplace=True)\n",
    "    \n",
    "df02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df02.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate col_name, add column tgt.\n",
    "df2 = df02.copy()\n",
    "\n",
    "# df2['CSN_SUM'] = df2['Convertible Senior Notes'].values.sum()\n",
    "# del df2['Convertible Senior Notes']\n",
    "\n",
    "df2['DeferredRevenue'] = df2['Deferred Revenue'].values.sum()\n",
    "del df2['Deferred Revenue']\n",
    "\n",
    "df2['NoncontrollingInterests'] = df2['Noncontrolling interests in subsidiaries'].values.sum()\n",
    "del df2['Noncontrolling interests in subsidiaries']\n",
    "\n",
    "#df2.set_index(['Year', 'Quarter'], inplace=True)\n",
    "\n",
    "print( df2.shape )\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_duplicate_columns( df2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ = pd.concat([df2, df01], axis=0, sort=False)\n",
    "\n",
    "print(c_.shape)\n",
    "c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serious Stuff. \n",
    "def duplicate_columns( DataF ):\n",
    "    \n",
    "    duplicates = [k for (k,v) in Counter(DataF.columns).items() if v > 1] \n",
    "\n",
    "    for dupli in duplicates: \n",
    "        \n",
    "        res = DataF[dupli].values.sum()\n",
    "        \n",
    "        DataF['Serious {}'.format(dupli)] = res\n",
    "        \n",
    "        DataF.drop(dupli, axis=1, inplace=True)\n",
    "\n",
    "    return DataF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for checking.\n",
    "def checking_duplicate_columns( DataF ):\n",
    "    \n",
    "    duplicates = [k for (k,v) in Counter(DataF.columns).items() if v > 1] \n",
    "\n",
    "    \n",
    "    print('Shape of df: ', DataF.shape )\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_.set_index(['Year', 'Quarter'], inplace=True)\n",
    "print(c_.shape)\n",
    "c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c1 = c_.copy()\n",
    "c1.dropna( axis= 1, inplace=True)\n",
    "print(c1.shape)\n",
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "QorK = {\n",
    "    '10-Q': ['0001564590-20-019931', '0001564590-19-038256', '0001564590-19-026445', '0001564590-19-013462', '0001564590-18-026353', '0001564590-18-019254', '0001564590-18-011086', '0001564590-17-021343', '0001564590-17-015705', '0001564590-17-009968', '0001564590-16-026820', '0001564590-16-023024', '0001564590-16-018886', '0001564590-15-009741', '0001564590-15-006666', '0001564590-15-003789'],\n",
    "    '10-K': ['0001564590-20-004475', '0001564590-19-003165', '0001564590-18-002956', '0001564590-17-003118', '0001564590-16-013195', '0001564590-15-001031']}\n",
    "QorK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating all Balance Sheet together.\n",
    "\n",
    "for k, v in QorK.items():\n",
    "    \n",
    "    #if k == '10-Q' and v == '0001564590-17-009968':\n",
    "        \n",
    "        for i, vv in enumerate(v):\n",
    "\n",
    "            # first grab 10-Q documents only\n",
    "            pages = master_filings_dict[v[i]]['filing_documents'][k]['pages_code']\n",
    "\n",
    "            try:\n",
    "                # Extract Table from html code and wrap it as a list of df.\n",
    "                table_df = table(pages)                 \n",
    "\n",
    "            except: \n",
    "                print('Something went wrong in file {}, file type = {}'.format(v[i], k))\n",
    "\n",
    "            with open('{}-{}.csv'.format(v[i], k), \"wb\") as fp:   # Pickling\n",
    "                pickle.dump(table_df, fp)\n",
    "                \n",
    "                  \n",
    "pd.set_option('display.max_columns', 100)\n",
    "with open(\"0001564590-15-001031-10-K.csv\", \"rb\") as fp:\n",
    "    file = pickle.load(fp)\n",
    "df1 = file[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statement Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS10_K(df)\n",
    "    \n",
    "    missing_values = [ '\\n', None, '\\n', '\\n0', '\\n$', '\\n()']\n",
    "    \n",
    "    # Clean df.\n",
    "    df = df.drop([1,4,5,6,7,8], axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF10_K(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoO10_K(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS10_Q(table):\n",
    "    \n",
    "    BSdf = []\n",
    "    missing_values = [ '\\n', None, '\\n', '\\n0', '\\n$', '\\n()']\n",
    "    \n",
    "    # Clean the table. \n",
    "    \n",
    "    row = table.replace(missing_values, '', regex=True)\n",
    "    file = file.drop(axis=1, columns = [1,4:], inplace=True)\n",
    "\n",
    "    statement = BSddf.DataFrame(row)\n",
    "    BSdf.append(statement)\n",
    "    \n",
    "    return BSdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF10_Q(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_table = []\n",
    "for table_num, table in enumerate(file):\n",
    "    \n",
    "    if table_num > 1:\n",
    "        print(table_num)\n",
    "        cleaned_row = cleaning_row(table)\n",
    "\n",
    "        res_table.append(cleaned_row)\n",
    "\n",
    "print(res_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Statment specific files for \n",
    "TSLA_BS10_K = []\n",
    "TSLA_CF10_K = []\n",
    "TSLA_SoO10_K = []\n",
    "TSLA_BS10_Q = []\n",
    "TSLA_CF10_Q = []\n",
    "TSLA_SoO10_Q = []\n",
    "\n",
    "for k, v in QorK.items():\n",
    "    \n",
    "    if k == '10-K':\n",
    "        \n",
    "        for i in range(len(v)):\n",
    "            \n",
    "    if k == '10-Q'\n",
    "    \n",
    "        for i in range(len(v)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/50950614/converting-column-into-multi-index-column"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
