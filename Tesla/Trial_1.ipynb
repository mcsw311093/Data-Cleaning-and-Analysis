{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library/Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "wb = load_workbook('foo.xlsx') # use the actual path of your workbook\n",
    "ws = wb['Bar'] # use your sheet name instead of Bar\n",
    "\n",
    "# iterate over all the rows in the sheet\n",
    "for row in ws: \n",
    "    # use the row only if it has not been filtered out (i.e., it's not hidden)\n",
    "    if ws.row_dimensions[row[0].row].hidden == False:\n",
    "        print row # ...or do what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse \n",
    "\n",
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams.update({'figure.figsize': (10, 7), 'figure.dpi': 120})\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Scikit-Learn's make_pipeline function\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Scikit-Learn's StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scikit-Learn's LinearRegression algorithm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Regularized Regression algos\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "# Import Tree Ensemble algos\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Import model metrics. \n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "quandl.ApiConfig.api_key = 'HybxHtVE5n-VgTJmhouM'\n",
    "data = quandl.get('NSE/OIL')\n",
    "#print( data.head())\n",
    "print( data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl.get_table('SHARADAR/SF1', qopts={\"columns\":['ticker' ,'dimension','datekey','revenue']}, ticker='AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl.get_table('SHARADAR/INDICATORS', table='SF1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl.get_table('SHARADAR/INDICATORS', indicator='revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEC - EDGAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#num = pd.read_csv('num.csv')\n",
    "#print(num.coreg.unique)\n",
    "num = open('num.txt', 'r')\n",
    "if num.mode == 'r':\n",
    "    contents = num.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import glob\n",
    "path = r'C:\\Users\\Matthew\\Documents\\Machine Learning Accelerator\\Capstone Project\\1 Predict Stock Price\\SEC_EDGAR\\2019q2'\n",
    "all_files = glob.glob(path+ \"/*.txt\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    with open(filename, newline='') as files:\n",
    "        file_reader = csv.reader(files, delimiter='\\t')\n",
    "        for file in file_reader:\n",
    "            df = li.append(dict(file))\n",
    "            sec = pd.DataFrame(df)\n",
    "            print( '{}: \\n {}'.format(filename, file) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = r'C:\\Users\\Matthew\\Documents\\Machine Learning Accelerator\\Capstone Project\\1 Predict Stock Price\\Internal'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=0, header=0, parse_dates=True, infer_datetime_format=True)\n",
    "    df = df.transpose(copy = True)\n",
    "    name = str(filename).strip(path)\n",
    "    df['Financial_Statement'] = name[:-4]\n",
    "\n",
    "    li.append(df)\n",
    "    print(name)\n",
    "    # Concatenate all data into one DataFrame\n",
    "internal_raw = pd.concat(li, axis=1, ignore_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dtype of Financial Statment to Category.\n",
    "internal_raw['Financial_Statement'] = internal_raw['Financial_Statement'].astype('category')\n",
    "# Double Check above.\n",
    "internal_raw['Financial_Statement'].dtypes\n",
    "# Change Filename of Financial Statements.\n",
    "internal_raw['Financial_Statement'] = internal_raw['Financial_Statement'].replace(['Flow_Sheet_10yr_Qtrly', '_Sheet_10yr_Qtrly'], ['Cash_Flow_Sheet_10yr_Qtrly', 'Income_Sheet_10yr_Qtrly'])\n",
    "internal_raw['Financial_Statement'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables into indicator features.\n",
    "internal_raw['Financial_Statement_cat'] = internal_raw['Financial_Statement'].cat.codes\n",
    "internal_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "internal_raw['Financial_Statement'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_raw.index.name = 'Date'\n",
    "#internal_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_raw['Date'] = pd.to_datetime(internal_raw.index, format='%d-%m-%y')\n",
    "internal_raw['Day'] = internal_raw['Date'].dt.day\n",
    "internal_raw['Month'] = internal_raw['Date'].dt.month\n",
    "internal_raw['Year'] = internal_raw['Date'].dt.year\n",
    "internal_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(internal_raw['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_raw.groupby('Month').agg(['min', 'median', 'mean', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Null values along columns (axis=1)\n",
    "internal_raw[internal_raw.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pandas import Grouper\n",
    "\n",
    "groups = internal_raw.groupby(Grouper(key='Month', axis=1))\n",
    "years = internal_raw\n",
    "for name, group in groups:\n",
    "    years[name.year] = group.values\n",
    "years.plot(subplots=True, legend=False)\n",
    "plt.figure(figsize=(60,30), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import glob\n",
    "\n",
    "path = r'C:\\Users\\Matthew\\Desktop\\HSBC MPF 2019' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    df = pd.read_csv(filename, index_col=0, header=0)\n",
    "    \n",
    "    df = df.transpose(copy = True)\n",
    "    Mask = df['Constituent Fund'] == 'BID'\n",
    "    df = df[Mask]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        df['{}'.format(col)] = df['{}'.format(col)].str.rstrip('\\t')\n",
    "        df.index = df.index.str.lstrip('\\t')\n",
    "        \n",
    "    li.append(df)\n",
    "    # Concatenate all data into one DataFrame\n",
    "frame = pd.concat(li, axis=0, ignore_index=False)\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "Internal --> External Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical description\n",
    "internal_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "internal_raw.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "internal_raw.set_index(['Year','Month'], inplace=True, verify_integrity=False)\n",
    "internal_raw.sort_index(inplace=True,na_position='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_raw.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Cleaning\n",
    "Internal --> External Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "Internal --> External Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assets(cash, accounts receivable) \n",
    "\n",
    "\n",
    "# Liabilities(expense & debt) \n",
    "\n",
    "\n",
    "# Shareholder equity(equity capital invest., retained earnings from periodic net income)\n",
    "\n",
    "\n",
    "# Balance: (Assets - Liabilities = Shareholder’s Equity = Book Value)\n",
    "\n",
    "\n",
    "# Liquidation Value = Market Price to Book ratio \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income Statement\n",
    "\n",
    " **Profit margin helps to show where company costs are low or high at different points of the operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue - Direct Costs = Gross Profit (margin = /Revenue)\n",
    "\n",
    "\n",
    "# Gross Profit - Indirect Expenses = Operating Profit (margin = /Revenue)\n",
    "\n",
    "# Operating Profit - Interest & Taxes = Net Profit (margin = /Revenue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cash Flow Statement\n",
    "**Bottom line = how much cash available in company**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating activities, CF + Net Income \n",
    "\n",
    "\n",
    "# Investing activities, CF + Firmwide Investments\n",
    "\n",
    "\n",
    "# Financing activities, CF + Debt & Equity Financing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evalutation \n",
    "Internal compare External Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library \n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: CIK: HTML\n",
    "company = {}\n",
    "company['auto'] = {}\n",
    "file_url = {\n",
    "    '0001318605' : {},\n",
    "    '37996' : {}\n",
    "}\n",
    "\n",
    "#print(company['name'])\n",
    "file_code = {}\n",
    "file_text = {}\n",
    "\n",
    "auto = {'Tesla' : '0001318605',\n",
    "    'Ford' : '37996'\n",
    "       }\n",
    "        \n",
    "company['auto'] = auto\n",
    "\n",
    "CIK = {\n",
    "    '0001318605' : '{}',\n",
    "    '37996' : '{}'\n",
    "    }\n",
    "\n",
    "\n",
    "key = list(CIK.copy().keys())\n",
    "\n",
    "\n",
    "print('Company Dictionary', company)\n",
    "print('-'*40)\n",
    "\n",
    "print( 'Auto: {}'.format(auto ) )\n",
    "\n",
    "print('-'*80)\n",
    "\n",
    "# List of CIK\n",
    "print( 'CIK-Keys: {}'.format(key) )\n",
    "print('-'*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Directory For CIK    \n",
    "dir_url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-Q&dateb=&owner=include&count=100'\n",
    "dir_url_list = [dir_url.format(x) for x in key]\n",
    "\n",
    "#print('Directory URL: {}'.format(dir_url_list))\n",
    "doc_url_list = [] \n",
    "# FOR-loop yielding Accession Numbers from CIK/URL Directory.\n",
    "for CIK_num in key:\n",
    "\n",
    "    doc_url = r'https://www.sec.gov/Archives/edgar/data/{CIKx}/{xx}/{yy}.txt'\n",
    "    doc_url_new = doc_url.format(CIKx = CIK_num, xx='{xx}', yy ='{yy}')\n",
    "    doc_url_list.append(doc_url_new)\n",
    "print(doc_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, value in enumerate(doc_url_list), :\n",
    "    print('num=', num, ':', 'value=', value)\n",
    "    print(type(num), type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for url in dir_url_list:\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    text = soup.get_text(strip=True)\n",
    "\n",
    "    cleaned_text = re.findall('Acc-no: \\d+-\\d+-\\d+' , text)\n",
    "\n",
    "    accession_number = [n.replace('Acc-no: ', '') for n in cleaned_text]\n",
    "    accessionnumber = [num.replace('-', '') for num in accession_number]\n",
    "\n",
    "    accession_numbers = zip(accessionnumber, accession_number)\n",
    "    \n",
    "    cikk = [cikk_.replace('CIK=', '') for cikk_ in re.findall('CIK=\\d+', url)][0]\n",
    "    CIKK = {cikk : accessionnumber}\n",
    "    CIK.update(CIKK)\n",
    "    #pprint(company['auto']['CIK'])\n",
    "    print('-'*80)\n",
    "    \n",
    "    for (a,b) in accession_numbers: \n",
    "            \n",
    "            doc_url_single = doc_url_list[key.index(cikk)].format(xx = a, yy = b)\n",
    "            \n",
    "            #Checking extracted URL CIK number matches with in-house Dictionary CIK number.\n",
    "            rotat = key.index(cikk)\n",
    "            #print('rotat: ', rotat)\n",
    "            doc_url_str = doc_url_list[ rotat ]\n",
    "            #print(doc_url_str, type(doc_url_str))\n",
    "            cik = [data.replace('/data/','') for data in re.findall('/data/\\d+', doc_url_str )][0].strip()\n",
    "            #print('cik', cik, ':', 'cikk', cikk)\n",
    "\n",
    "            file_url_list = []\n",
    "            Accession_Number_URL = {}\n",
    "\n",
    "\n",
    "            if (cikk == cik) == True:\n",
    "            \n",
    "                file_url_list.append( doc_url_single )\n",
    "                Accession_Number_URL.update({ a : file_url_list})\n",
    "                \n",
    "                file_url[cikk].update(Accession_Number_URL)\n",
    "                \n",
    "                \n",
    "                #print('cik{}: {}'.format((rotat +1), cik))  \n",
    "                \n",
    "            else:\n",
    "                print('An error occured while checking!', 'cik: ', cik, 'cikk: ', cikk, 'Accession_Number: ', a)\n",
    "                print('~~~~~'*20)\n",
    "                \n",
    "            del file_url_list\n",
    "            del b\n",
    "            \n",
    "             \n",
    "            del Accession_Number_URL\n",
    "                #company['auto']['CIK'] = Accession_Number_URL\n",
    "print(file_url)\n",
    "#pprint(CIK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop and parse (bs4) through each filing to extract 10-Q or 10-K Document. \n",
    "\n",
    "# Product = doc_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through doc_code to .append() tables <td>/<tr>. \n",
    "\n",
    "# Product = doc_code_table, save table as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop and normalise text in doc_code_table.\n",
    "\n",
    "# Product = cleaned_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = {\n",
    "    'dog' : 1, \n",
    "    'cat' : 2, \n",
    "    'mice' : 3, \n",
    "    'deer' : 4,\n",
    "    'lupus' : {}\n",
    "}\n",
    "print('land: ', land)\n",
    "sea = {\n",
    "    'fish' : 5,\n",
    "    'shark' : 6, \n",
    "    'octopus' : 7,\n",
    "}\n",
    "print('sea: ', sea)\n",
    "car = ['tes', 'fm', 'gp', 'tm' ]\n",
    "list_4 = ['f','o','u','r']\n",
    "\n",
    "app = zip(car, list_4)\n",
    "print(app)\n",
    "\n",
    "l3 = [ 6, 7, 8, 5 ]\n",
    "#l4 = [0, 'f','o','u','r']\n",
    "d3 = {'rick':'morty'}\n",
    "norm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "land['lupus'] = {'wolf' : 5.0,\n",
    "                'dog' : 6.0}\n",
    "print(land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value1_2 = zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in land.values():\n",
    "    if type(value) == dict:\n",
    "        for value in value:\n",
    "            print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value_1 in dict(land.values()).values():\n",
    "    if value_1.dtype() == dict:\n",
    "        print(value_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in app:\n",
    "    print('a:', a)\n",
    "    \n",
    "    print('b:',b)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land.update(sea)\n",
    "print('land: ', land)\n",
    "print('app: ', app)\n",
    "print(d3['rick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.update(land)\n",
    "print('update empty dict with dict: \\n sea-', sea)\n",
    "print('-'*40)\n",
    "app = zip(car, list_4)\n",
    "def new_dict(l3, app):\n",
    "    for c in l3:\n",
    "        print(c)\n",
    "        print('-'*30)\n",
    "\n",
    "        for a,b in app:\n",
    "            new_norm = {a:c for a,c in app} \n",
    "            print(new_norm)\n",
    "            norm.update( new_norm )\n",
    "            print('updated norm: ', norm)\n",
    "\n",
    "new_dict(l3, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l3.index(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_table = 'https://www.sec.gov/Archives/edgar/data/0001318605/000156459019038256/0001564590-19-038256.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import lxml\n",
    "from lxml import html\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = html.fromstring(html_table)\n",
    "tree.xpath(\"//h1/text()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_html(html_table)\n",
    "print(df[0].dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library \n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_html = 'https://www.sec.gov/Archives/edgar/data/1318605/000156459019038256/0001564590-19-038256.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72']\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(some_html)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "filing_doc_text = soup.find('text').extract()\n",
    "all_thematic_breaks = filing_doc_text.find_all('hr')\n",
    "all_page_numbers = [thematic_break.previous_sibling.previous_sibling.get_text(strip=True) for thematic_break in all_thematic_breaks]\n",
    "print(all_page_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_document_dict = {}\n",
    "master_filings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the documents¶\n",
    "## After optional part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "There was 72 page(s) found.\n",
      "There was 72 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    " # prep the document text for splitting, this means converting it to a string.\n",
    "filing_doc_string = str(filing_doc_text) \n",
    "    \n",
    "    # handle the case where there are thematic breaks.\n",
    "if len(all_thematic_breaks) > 0:\n",
    "    \n",
    "        # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "        regex_delimiter_pattern = '|'.join(map(re.escape, str(all_thematic_breaks)))\n",
    "\n",
    "        # split the document along each thematic break.\n",
    "        split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "        # store the document itself\n",
    "        master_document_dict['pages_code'] = split_filing_string\n",
    "\n",
    "    # handle the case where there are no thematic breaks.\n",
    "elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "        # handles so it will display correctly.\n",
    "        split_filing_string = all_thematic_breaks\n",
    "        \n",
    "        # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "        master_document_dict['pages_code'] = [filing_doc_string]\n",
    "    \n",
    "\n",
    "    # display some information to the user.\n",
    "print('-'*80)\n",
    "#print('The document {} was parsed.'.format(document_id))\n",
    "print('There was {} page(s) found.'.format(len(all_page_numbers)))\n",
    "print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "    \n",
    "\n",
    "# store the documents in the master_filing_dictionary.\n",
    "master_filings_dict['filing_documents'] = master_document_dict\n",
    "\n",
    "print('-'*80)\n",
    "#print('All the documents for filing {} were parsed and stored.'.format(accession_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Pulling document pages_code for text normilzation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'INDEX'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict['filing_documents']\n",
    "\n",
    "\n",
    "    # loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # display some info to give status updates.\n",
    "    print('-'*80)\n",
    "    print('Pulling document {} for text normilzation.'.format(document_id))\n",
    "    \n",
    "    # grab all the pages for that document\n",
    "    document_pages = filing_documents['pages_code']\n",
    "    \n",
    "    # page length\n",
    "    pages_length = len(filing_documents['pages_code'])\n",
    "    \n",
    "    # initalize a dictionary that'll house our repaired html code for each page.\n",
    "    repaired_pages = {}\n",
    "    \n",
    "    # initalize a dictionary that'll house all the normalized text.\n",
    "    normalized_text = {}\n",
    "\n",
    "    # loop through each page in that document.\n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser. NOTE I AM USING THE HTML5 PARSER. YOU MUST USE THIS TO FIX BROKEN TAGS.\n",
    "        page_soup = BeautifulSoup(page,'html5')\n",
    "        \n",
    "        # grab all the text, notice I go to the BODY tag to do this\n",
    "        page_text = page_soup.html.body.get_text(' ',strip = True)\n",
    "        \n",
    "        # normalize the text, remove messy characters. Additionally, restore missing window characters.\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text)) \n",
    "        \n",
    "        # Additional cleaning steps, removing double spaces, and new line breaks.\n",
    "        page_text_norm = page_text_norm.replace('  ', ' ').replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #####################################\n",
    "    # THIS WILL HANDLE THE TABLE SEARCH #\n",
    "    #####################################\n",
    "    \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store matching words.\n",
    "    tables_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the tables\n",
    "        tables_found = page_code.find_all('table')\n",
    "        \n",
    "        # number of tables found\n",
    "        num_found = len(tables_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the tables found.\n",
    "        tables_dict[page_num] = {(table_id + 1): table for table_id, table in enumerate(tables_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} tables.'.format(page_num, page_length, document_id, num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for tables.'.format(document_id)) \n",
    "    print('-'*80)    \n",
    "    \n",
    "        \n",
    "    # let's add the matching words dict to the document.\n",
    "    filing_documents[document_id]['word_search'] = matching_words_dict  \n",
    "    \n",
    "    # let's add the matching tables dict to the document.\n",
    "    filing_documents[document_id]['table_search'] = tables_dict\n",
    "    \n",
    "    # let's add the matching anchors dict to the document.\n",
    "    filing_documents[document_id]['anchor_search'] = link_anchor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_dictionary(table_dictionary):\n",
    "    \n",
    "    # initalize a new dicitonary that'll house all your results\n",
    "    new_table_dictionary = {}\n",
    "    \n",
    "    if len(table_dictionary) != 0:\n",
    "\n",
    "        # loop through the dictionary\n",
    "        for table_id in table_dictionary:\n",
    "\n",
    "            # grab the table\n",
    "            table_html = table_dictionary[table_id]\n",
    "            \n",
    "            # grab all the rows.\n",
    "            table_rows = table_html.find_all('tr')\n",
    "            \n",
    "            # parse the table, first loop through the rows, then each element, and then parse each element.\n",
    "            parsed_table = [\n",
    "                [element.get_text(strip=True) for element in row.find_all('td')]\n",
    "                for row in table_rows\n",
    "            ]\n",
    "            \n",
    "            # keep the original just to be safe.\n",
    "            new_table_dictionary[table_id]['original_table'] = table_html\n",
    "            \n",
    "            # add the new parsed table.\n",
    "            new_table_dictionary[table_id]['parsed_table'] = parsed_table\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing '$'.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != '$']\n",
    "                for row in parsed_table\n",
    "            ]\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing Blanks.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != None]\n",
    "                for row in parsed_table_cleaned]\n",
    "\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # if there are no tables then just have the id equal NONE\n",
    "        new_table_dictionary[1]['original_table'] = None\n",
    "        new_table_dictionary[1]['parsed_table'] = None\n",
    "        \n",
    "    return new_table_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_table_dictionary(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
